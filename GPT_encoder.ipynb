{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf2r8jnUyPJ_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GcQH_pT1dkS",
        "outputId": "be248e31-cd5a-402f-d7a9-93ac7574a35d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115395\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of all unique characters that occur in the text"
      ],
      "metadata": {
        "id": "5KUhWqgc12-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3ZJuTkH1q3c",
        "outputId": "ff3a0a78-d25d-4c35-ac7c-499d990a25b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping characters to integers and vice versa"
      ],
      "metadata": {
        "id": "PQ9_Yohs2YTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"encode this sentence\"))\n",
        "print(decode(encode(\"encode this sentence\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1HqmV3-2V5b",
        "outputId": "6ae4beb4-a081-4fa1-fc13-ed139349b392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[43, 52, 41, 53, 42, 43, 1, 58, 46, 47, 57, 1, 57, 43, 52, 58, 43, 52, 41, 43]\n",
            "encode this sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "id": "osmK6zba4UCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d401a532-b789-4160-b7cc-9af9042cb1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0bb3511170>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lets encode whole text and store in torch tensors\n",
        "- Train and validation data split"
      ],
      "metadata": {
        "id": "hV6G6GbR28Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "qwwPj_X226-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "iOuFUXa2-dSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Function to generate random batch and returns inputs x and targets y.\n",
        "\n",
        "- For any character at index 'i' in target vector y, is considered for sequence of characters from 0 to 'i' in input vector x."
      ],
      "metadata": {
        "id": "4zhqtjWw43FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "9XgUfX9s-EPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every batch can be more or less lucky. So, better way is to calculate average loss for 'eval_iters' number of batches."
      ],
      "metadata": {
        "id": "8fLcr8n4AuZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't want to store intermediate variables produced during backward() operations for memory saving\n",
        "@torch.no_grad() # Below function will not perform backpropagation\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    #Some layers might have different behaviour at inference and training time.\n",
        "    #So, it is better idea to set model on eval mode\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # Setting model back to train mode\n",
        "    return out"
      ],
      "metadata": {
        "id": "mYseRpPT3NxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single head for self attention"
      ],
      "metadata": {
        "id": "oU8hgkJSHrbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "cgU79uSBHkPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since multi head attention require lesser number of layers than single head attention when attending the same numbers of positions, it provides training stability."
      ],
      "metadata": {
        "id": "OGj454Asd0D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "BHlY3MP93Svp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "jni9r1RWMjAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LayerNorm: (Batch Normalization)\n",
        " $ y = ((x−E[x])/​(Var[x]+ϵ))​∗γ+β $\n",
        "\n"
      ],
      "metadata": {
        "id": "ZtBEg9FSgSOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "CDtf7l-VMqMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "pk_mHJyxNIn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puKzSgkoaB-U",
        "outputId": "e8f153ce-9525-4bbd-d302-2ac4af615810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OTxS3VuaEFi",
        "outputId": "b5d0946c-f414-4fce-b50a-b98f19dd48ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3246, val loss 4.3240\n",
            "step 100: train loss 2.6667, val loss 2.6678\n",
            "step 200: train loss 2.5056, val loss 2.5236\n",
            "step 300: train loss 2.4304, val loss 2.4366\n",
            "step 400: train loss 2.3569, val loss 2.3679\n",
            "step 500: train loss 2.3154, val loss 2.3186\n",
            "step 600: train loss 2.2649, val loss 2.2692\n",
            "step 700: train loss 2.2243, val loss 2.2342\n",
            "step 800: train loss 2.1848, val loss 2.2180\n",
            "step 900: train loss 2.1310, val loss 2.1728\n",
            "step 1000: train loss 2.0962, val loss 2.1387\n",
            "step 1100: train loss 2.0762, val loss 2.1218\n",
            "step 1200: train loss 2.0472, val loss 2.0965\n",
            "step 1300: train loss 2.0183, val loss 2.0749\n",
            "step 1400: train loss 1.9990, val loss 2.0735\n",
            "step 1500: train loss 1.9660, val loss 2.0333\n",
            "step 1600: train loss 1.9571, val loss 2.0304\n",
            "step 1700: train loss 1.9510, val loss 2.0263\n",
            "step 1800: train loss 1.9230, val loss 2.0169\n",
            "step 1900: train loss 1.8949, val loss 1.9910\n",
            "step 2000: train loss 1.8790, val loss 1.9914\n",
            "step 2100: train loss 1.8763, val loss 1.9804\n",
            "step 2200: train loss 1.8577, val loss 1.9645\n",
            "step 2300: train loss 1.8489, val loss 1.9519\n",
            "step 2400: train loss 1.8401, val loss 1.9368\n",
            "step 2500: train loss 1.8215, val loss 1.9425\n",
            "step 2600: train loss 1.8121, val loss 1.9350\n",
            "step 2700: train loss 1.8027, val loss 1.9362\n",
            "step 2800: train loss 1.7889, val loss 1.9057\n",
            "step 2900: train loss 1.7940, val loss 1.9245\n",
            "step 3000: train loss 1.7663, val loss 1.9050\n",
            "step 3100: train loss 1.7690, val loss 1.9095\n",
            "step 3200: train loss 1.7543, val loss 1.8917\n",
            "step 3300: train loss 1.7547, val loss 1.8932\n",
            "step 3400: train loss 1.7369, val loss 1.9003\n",
            "step 3500: train loss 1.7365, val loss 1.8802\n",
            "step 3600: train loss 1.7301, val loss 1.8785\n",
            "step 3700: train loss 1.7139, val loss 1.8769\n",
            "step 3800: train loss 1.7100, val loss 1.8740\n",
            "step 3900: train loss 1.7124, val loss 1.8619\n",
            "step 4000: train loss 1.7052, val loss 1.8607\n",
            "step 4100: train loss 1.6995, val loss 1.8620\n",
            "step 4200: train loss 1.6823, val loss 1.8449\n",
            "step 4300: train loss 1.6923, val loss 1.8393\n",
            "step 4400: train loss 1.6823, val loss 1.8452\n",
            "step 4500: train loss 1.6695, val loss 1.8381\n",
            "step 4600: train loss 1.6760, val loss 1.8403\n",
            "step 4700: train loss 1.6703, val loss 1.8230\n",
            "step 4800: train loss 1.6615, val loss 1.8376\n",
            "step 4900: train loss 1.6717, val loss 1.8360\n",
            "step 4999: train loss 1.6663, val loss 1.8191\n",
            "\n",
            "Will in say infettence: that is shall me to to me\n",
            "This is our suie and of out thin straire her.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Which that now, fare you,\n",
            "For\n",
            "In Anten'd not hus. By when sman,\n",
            "'Twords, tee posurance, the foe this me the days\n",
            "All in the madned be many; stand men!\n",
            "Go usil! peace you? hill-good, in sunatue him.\n",
            "Murtues go, that four would From sit,\n",
            "Ward the recrows me. Fay care mumby, let you make, I madster year thus ear you knaves.\n",
            "O? in thee and chappy bithers. Wheria thee abs:\n",
            "Madgen curpagit but you in the dind,\n",
            "Medien me. Plagenough, me we their wip's 'not but propor on.\n",
            "\n",
            "MON:\n",
            "Buckmern those you won; '\n",
            "Corst my should this and again\n",
            "haste sort my shookes to down the lure of those curn with yet\n",
            "To hee, be all shall deaugh of Jucing unjitory,\n",
            "Ercamess me you king me those, my let;\n",
            "Daught of that as goot on her ween\n",
            "The see, were ars youte of you joy\n",
            "This sweat stagarin'd it and run in your kinter,\n",
            "Hors' not me the hand my donguest, would dot me;\n",
            "That yie at it, were you not is siage.\n",
            "\n",
            "WAM:\n",
            "Fand dater Rummorent; duke there's in my earth:\n",
            "Tynough,\n",
            "Whut is a goft and thee her accument's fake, hen excre.\n",
            "\n",
            "QUEEN HEN EEN EXq O?\n",
            "\n",
            "ARGAURE:\n",
            "Mand not man, Cost again. But,\n",
            "Clious, and our your so hair fels.\n",
            "\n",
            "CAMILLO:\n",
            "What plos Gars crease more the undlaguent than?\n",
            "\n",
            "Servus:\n",
            "Nowers buse, my my as do for you?\n",
            "\n",
            "ESCHUMIO:\n",
            "\n",
            "Purst Bercoods:\n",
            "He armply the more than, my must.\n",
            "\n",
            "CHORSSOY:\n",
            "Will swere you. O, suck you, and to meln I pasunes hink me to hour must his muld say:\n",
            "I say not hath now the were you.\n",
            "\n",
            "KING RICHARD:\n",
            "Is cleself, witor play man to hour sou's mustican\n",
            "Or men; not your down. hood were and us you.\n",
            "\n",
            "Fexp; I hold out in you.\n",
            "\n",
            "KING EDWER\n",
            "BUCK:\n",
            "You loved the sumbstains and underon.\n",
            "\n",
            "FLADY WARET:\n",
            "Apo ne yearry, or where Fear car\n",
            "I deharrd bunint my End you.\n",
            "\n",
            "QUEEN OF HAM:\n",
            "For come, ' and crown'd, you?\n",
            "\n",
            "Served Servy con; so who,\n",
            "Thereiry of the yell rope\n",
            "Faught inch the till to deeed of Whard\n",
            "And prising me? a goot unhumm.\n",
            "\n",
            "CORIOLANUS:\n",
            "Swenty fir with I do,\n",
            "But God himp you \n"
          ]
        }
      ]
    }
  ]
}